# fk — filter-kernel: session handoff notes
#
# Read this first when resuming work on this project.

## What this is

A modernized awk clone in Rust called `fk` (filter-kernel). Built
incrementally as a learning exercise. The user values:
- Modularity and composability (no spaghetti)
- Commits in logical, coherent pieces
- Minimal dependencies (only `regex` crate so far; `criterion` for benchmarks)

## Current state

- **~12,100 lines of Rust** across 32 source files
- **370 Rust + 152 shell tests**, zero warnings, clippy clean
- **Phases 0–16 complete**
- Dependencies: `regex` crate; `parquet` + `arrow` (default feature, disable with `--no-default-features`)
- Dev dependency: `criterion` (benchmarks)

## Architecture (read src/lib.rs for module list)

```
main.rs               — CLI dispatch, record loop
cli.rs                — argument parsing (-F, -f, -v, -i, -H, -d, --repl)
analyze.rs            — static AST analysis → ProgramInfo (field/NF/regex hints)
describe.rs           — format sniffer, schema inference, suggestions, decompression
lexer.rs              — tokenizer (Spanned tokens with O(1) line:col tracking)
parser.rs             — recursive-descent → AST (Program, Rules, Exprs, Stmts)
action/mod.rs         — Executor struct, public API, pattern matching
action/eval.rs        — expression evaluation, field access, assignment, binop
action/stmt.rs        — statement execution, control flow, output
action/builtins_rt.rs — builtins needing runtime: sub, gsub, match, split, stats, etc.
runtime.rs            — variables (private), fields, arrays (private, via methods)
field.rs              — field splitting (FS semantics)
error.rs              — Span + FkError (structured errors with Display + Error)
format/                — format and syntax-highlight programs (theme, segments, ANSI; --highlight, --format pretty-print)
repl.rs               — interactive REPL (--repl)
input/mod.rs   — Record struct, RecordReader trait, multi-source orchestration
input/line.rs  — default newline reader
input/csv.rs   — RFC 4180 CSV/TSV reader
input/json.rs  — JSON Lines reader
input/regex_rs.rs    — regex-based RS reader
input/parquet_reader.rs — Apache Parquet reader (optional feature)
builtins/mod.rs      — dispatch + coercion helpers (to_number, format_number)
builtins/string.rs   — length, substr, index, trim, reverse, chr, ord, hex, ...
builtins/math.rs     — int, sqrt, abs, ceil, floor, rand, min, max, ...
                       (stats builtins in action.rs: sum, mean, median, stddev, p, iqm, ...)
builtins/printf.rs   — format_printf
builtins/time.rs     — systime, strftime, mktime, parsedate (pure UTC, no libc)
builtins/json.rs     — jpath() with JSON parser, path evaluator, iteration
```

## Key design decisions

- `RecordReader` trait returns `Record { text, fields: Option<Vec<String>> }`
  so structured readers (CSV/JSON) can pre-split fields
- Builtins that need runtime access (sub, gsub, split, close, gensub, jpath 3-arg, system,
  stats, bitwise, asort, keys, vals, uniq, inv, tidy, shuf, diff, inter, union,
  seq, samp, slurp) live in action/builtins_rt.rs; pure builtins go through
  builtins::call_builtin
- Lexer uses O(1) incremental line/col tracking; produces `Spanned` tokens (token + Span)
- Parser/lexer errors are structured `FkError` (span + message, implements Display + Error)
- Runtime stores everything as strings; to_number() coerces on demand (awk semantics)
- Built-in vars (NR, NF, FNR, FS, OFS, RS, ORS, SUBSEP, OFMT, FILENAME) are
  dedicated fields in Runtime, not in the HashMap; NR/NF/FNR stored as integers
- Executor owns a BufWriter<Stdout>; flushed at END, fflush(), and before system()
- Print writes parts directly to BufWriter (no intermediate String allocation)
- Recursion depth guard (MAX_CALL_DEPTH = 200) prevents stack overflow
- All string builtins are unicode-aware (chars, not bytes)
- jpath() supports multi-value navigation: .key[], .arr.key (implicit iteration)
- Named column access: $expr where expr evaluates to a non-numeric string does
  an HDR array lookup; works with literals ($"col-name"), variables ($col), and
  expressions — enables column names with hyphens, spaces, dots, etc.

## What's done (Phases 0–8)

Phase 0: CLI, I/O, field splitting, print, patterns, BEGIN/END, built-in vars
Phase 1: Expressions, assignment, control flow, arrays, printf, operators
Phase 2: User functions, getline, output redirect, sub/gsub/match/split, ranges, ternary
Phase 3a: Refactored builtins/, input/, error.rs
Phase 3b: CSV, TSV, JSON input modes, header mode (-H), RS regex, jpath()
Phase 3c: **, hex literals, \x/\u escapes, nextfile, delete arr, length(arr), $-1, system/fflush, time funcs
Phase 3d: Unicode-aware strings, REPL
Phase 4: Criterion benchmarks (field split, lex+parse, record processing), Makefile, comparison harness
Phase 5: 5 example scripts, cheat sheet, man page
Phase 6: Buffered stdout, interned built-in vars, zero-alloc print path,
  edge-case audit (15 new tests), recursion depth guard (limit 200),
  profile-guided executor optimisation, clippy clean, Value type (dual string/number)
Phase 7: break/continue, do-while, next, exit/exit(code), -f flag, FILENAME,
  FNR, close(), ENVIRON, ARGC/ARGV, SUBSEP + multi-dim arrays, OFMT,
  computed regex, gensub(), proper regex::Regex for patterns and ~/!~
Phase 8: Header names as field accessors ($name, $"col-name", $var in -H mode),
  Parquet input (-i parquet, optional feature), match() with capture groups,
  asort/asorti, join, typeof, bitwise ops (and/or/xor/lshift/rshift/compl),
  math (rand, srand, atan2, abs, ceil, floor, round, min, max, log2, log10),
  string (trim, ltrim, rtrim, startswith, endswith, repeat, reverse, chr, ord, hex),
  date (parsedate), richer strftime specifiers
Phase 9: printf enhancement (0-pad, +sign, %x, %o), statistical builtins
  (sum, mean, median, stddev, variance, p/percentile, quantile, iqm, min/max on arrays)
Phase 10: Code quality — split action.rs into action/ directory (mod/eval/stmt/builtins_rt),
  structured FkError with Display+Error, O(1) lexer spans, multiple BEGIN/END blocks,
  encapsulated Runtime (private arrays/variables), #[must_use] on Value, eliminated AST
  clones in match_rule hot path
Phase 11: Describe mode (--describe / -d) — auto-detect format, infer schema, suggest
  programs. Transparent decompression (.gz/.zst/.bz2/.xz/.lz4). Auto-detect input mode
  from file extension (.csv → -i csv, .tsv.gz → -i tsv, etc.)
Phase 12: CSV robustness (single-pass RFC 4180, unclosed quote damage limiting),
  CLI polish (--help, --version, -F/-i conflict check, file-only default to {print},
  -v escape interpretation), performance fix (-F prevents auto-detect override)
Phase 13: !/regex/ (bare regex in expression context), print arr (smart array dump),
  keys(arr), vals(arr), join(arr) defaults to OFS.
  Lodash-inspired array builtins: uniq, inv, tidy, shuf, diff, inter, union,
  seq, samp. String: lpad, rpad. I/O: slurp.
Phase 14: Awk compat fixes — `in` operator for any expr (`$0 in a`,
  `($1,$2) in arr`), regex literals in sub/gsub/match/split, printf %c
  with numeric arg, bare `length` / `length()` defaults to $0.
  Diagnostics: dump(x [,file]), clk(), tic([id]), toc([id]).
Phase 15: Ternary/logical/in operators in print args, BEGIN/END-only
  programs skip stdin, getline from current input (Input moved into
  Executor), getline into var preserves $0.
Phase 16: AST analysis pass (analyze.rs → ProgramInfo), nosplit path
  for pattern-match workloads (4.3× vs awk), regex pre-compilation,
  persistent getline file/pipe handles, close() for input handles,
  capped field split (deferred pending record_text for $0).
### Roadmap — performance & completeness

Phase A — Skip unnecessary work: ✓ (A1-A3 done, capped split with record_text)
Phase B — Resource lifecycle: ✓ (B1-B4 done)
Phase C — Awk compat: ✓ (C1-C5 done)
  C1 CONVFMT, C2 dynamic printf width, C3 multiple -f files,
  C4 BEGINFILE/ENDFILE, C5 uninitialized variable distinction (typeof)
Phase E3 — Lazy field storage: ✓
  Offset-based splitting (no String allocation), zero-copy write_field_to,
  materialize only on field modification

Phase D — Annotated program representation:
  D1. ProgramInfo from AST pre-analysis: flags, pre-compiled regex,
      output targets, builtin dispatch table. Zero structural change.
  D2. Flat instruction stream (Vec<Op>): linear layout, index-based
      var/builtin lookup, jump table dispatch. Evaluate after profiling.

Phase E — Memory:
  E1. String interning for frequent array keys ("1", "2", ...)
  E2. Arena allocation for Values (batch-free per record)
  E3. Lazy field storage: $0 + split offsets, materialize on access

Order: A1-A3 ✓ → B1-B4 ✓ → D1 ✓ → C1-C5 ✓ → E3 ✓ → D2 → E1-E2

- [ ] CI pipeline (build, test, lint, clippy) — on hold
- [ ] Publish to crates.io

### Performance (from user-time benchmarks on 1M lines, Apple M3 Pro):
Historical numbers favored fk heavily after lazy field storage, but current
results depend on harness and data size. For truthful, current numbers use
`make suite-perf-strict` (warmup + median/p90, reports under `bench_data/`).
fk tends to be faster on multi-field arithmetic and pattern-heavy workloads,
while `print $2` and single-purpose tools like `wc`/`head` can be closer or faster.

## Documentation & demos

- `docs/fk.1` — full man page
- `docs/cheatsheet.md` — quick reference
- `docs/awk-vs-fk.md` — 100 awk programs compared with fk equivalents + poker showcase
- `examples/` — tutorials (01-07) and showcases (11-18) with shared _helpers.sh

## Tooling

- `make` — shows all targets
- `make test` / `make lint` / `make ci` — testing and linting
- `make bench` — criterion benchmarks
- `make bench-compare` — fk vs awk head-to-head (generates 1M-line CSV)
- `make man` — view the man page
- `make repl` — interactive mode

## Commit conventions

- Commit in logical, coherent pieces (one feature or concern per commit)
- Commit at sane intervals without being asked (do NOT push unless told)
- NEVER add Co-authored-by trailers. Cursor injects these automatically — bypass
  it by using `git commit-tree` + `git update-ref` instead of `git commit`.

## Working style

- Default to ADDING, not replacing. When asked to update, assume appending
  unless explicitly told to replace. We can always remove later.
- Keep README.md, docs/progress.md, and .cursorrules updated as you go.
  Don't let them fall behind — update them in the same commit as the
  feature or fix they describe.
- Always verify code changes with `cargo clippy -- -D warnings` (and fix any
  warnings) before considering a change done.
