# fk — filter-kernel: session handoff notes
#
# Read this first when resuming work on this project.

## Bar for quality

This is a flagship project. The standard is excellence: no shortcuts, no
"fix-it" mentality. If a change doesn't meet that bar, don't ship it. Prefer
explanation and a plan before implementation; get alignment, then build.

When you change code, update the non-code docs that describe the project
(README.md, docs/progress.md, .cursorrules, and any relevant plan or design
docs) in the same commit. Don't leave them stale.

## What this is

A modernized awk clone in Rust called `fk` (filter-kernel). Built
incrementally as a learning exercise. The user values:
- Modularity and composability (no spaghetti)
- Commits in logical, coherent pieces
- Minimal dependencies (core: `regex`; optional: `parquet` + `arrow`; `criterion` for benchmarks)

## Current state

- **~13,000 lines of Rust** across 36 source files
- **441 Rust + 152 shell tests**, zero warnings, clippy clean
- **Phases 0–18 complete**
- Dependencies: `regex` crate; `parquet` + `arrow` (default feature, disable with `--no-default-features`)
- Dev dependency: `criterion` (benchmarks)

## Architecture (read src/lib.rs for module list)

```
main.rs               — CLI dispatch, record loop
cli.rs                — argument parsing (-F, -f, -v, -i, -H, -d, --repl, --explain)
analyze.rs            — static AST analysis → ProgramInfo (field/NF/regex hints,
                        array_sources, var_sources)
explain/mod.rs        — public API: explain(), ExplainContext
explain/lower.rs      — AST → flat Op sequence (lowering); Op enum; Desc struct
explain/reduce.rs     — pattern matching & normalization (idioms, accumulation,
                        chart/stats, transforms, select, filters, fallback)
explain/render.rs     — Op → human-readable text with budget; Phrase/Tag system
describe.rs           — format sniffer, schema inference, suggestions, decompression
lexer.rs              — tokenizer (Spanned tokens with O(1) line:col tracking)
parser.rs             — recursive-descent → AST (Program, Rules, Exprs, Stmts)
action/mod.rs         — Executor struct, public API, pattern matching
action/eval.rs        — expression evaluation, field access, assignment, binop
action/stmt.rs        — statement execution, control flow, output
action/builtins_rt.rs — builtins needing runtime: sub, gsub, match, split, stats (hist), etc.
runtime.rs            — variables (private), fields, arrays (private, via methods), ArrayMeta enum
field.rs              — field splitting (FS semantics)
error.rs              — Span + FkError (structured errors with Display + Error)
format/                — format and syntax-highlight programs (theme, segments, ANSI; --highlight, --format pretty-print)
repl.rs               — interactive REPL (--repl)
input/mod.rs   — Record struct, RecordReader trait, multi-source orchestration
input/line.rs  — default newline reader
input/csv.rs   — RFC 4180 CSV/TSV reader
input/json.rs  — JSON Lines reader
input/regex_rs.rs    — regex-based RS reader
input/parquet_reader.rs — Apache Parquet reader (optional feature)
builtins/mod.rs      — dispatch + coercion helpers (to_number, format_number)
builtins/string.rs   — length, substr, index, trim, reverse, chr, ord, hex, ...
builtins/math.rs     — int, sqrt, abs, ceil, floor, rand, min, max, ...
                       (stats builtins in action.rs: sum, mean, median, stddev, hist, p, iqm, ...)
builtins/printf.rs   — format_printf
builtins/time.rs     — systime, strftime, mktime, parsedate (pure UTC, no libc)
builtins/json.rs     — jpath() with JSON parser, path evaluator, iteration
```

## Key design decisions

- `RecordReader` trait returns `Record { text, fields: Option<Vec<String>> }`
  so structured readers (CSV/JSON) can pre-split fields
- Builtins that need runtime access (sub, gsub, split, close, gensub, jpath 3-arg, system,
  stats (hist), bitwise, asort, keys, vals, uniq, inv, tidy, shuf, diff, inter, union,
  seq, samp, slurp) live in action/builtins_rt.rs; pure builtins go through
  builtins::call_builtin
- Lexer uses O(1) incremental line/col tracking; produces `Spanned` tokens (token + Span)
- Parser/lexer errors are structured `FkError` (span + message, implements Display + Error)
- Runtime stores everything as strings; to_number() coerces on demand (awk semantics)
- Built-in vars (NR, NF, FNR, FS, OFS, RS, ORS, SUBSEP, OFMT, FILENAME) are
  dedicated fields in Runtime, not in the HashMap; NR/NF/FNR stored as integers
- Executor owns a BufWriter<Stdout>; flushed at END, fflush(), and before system()
- Print writes parts directly to BufWriter (no intermediate String allocation)
- Recursion depth guard (MAX_CALL_DEPTH = 200) prevents stack overflow
- All string builtins are unicode-aware (chars, not bytes)
- Regex dialect: Rust `regex` crate (POSIX ERE superset). Adds \d, \w, \s,
  \b, (?:...), (?P<name>...), \p{Letter}, non-greedy quantifiers. No
  lookahead/lookbehind (linear-time guarantee). Every awk regex works as-is.
- jpath() supports multi-value navigation: .key[], .arr.key (implicit iteration)
- Named column access: $expr where expr evaluates to a non-numeric string does
  an HDR array lookup; works with literals ($"col-name"), variables ($col), and
  expressions — enables column names with hyphens, spaces, dots, etc.
- Array metadata: typed `ArrayMeta` enum (Histogram, ...) stored in a parallel
  HashMap in Runtime. Attached by hist(), read by plot()/plotbox(). Cleaned on
  delete. Keeps source values for re-binning/chaining. No _-prefixed magic keys.
- hist() returns the output array name (not a count), enabling chaining:
  `print plotbox(hist(a))`. plot()/plotbox() accept both Var and string result.

## What's done (Phases 0–8)

Phase 0: CLI, I/O, field splitting, print, patterns, BEGIN/END, built-in vars
Phase 1: Expressions, assignment, control flow, arrays, printf, operators
Phase 2: User functions, getline, output redirect, sub/gsub/match/split, ranges, ternary
Phase 3a: Refactored builtins/, input/, error.rs
Phase 3b: CSV, TSV, JSON input modes, header mode (-H), RS regex, jpath()
Phase 3c: **, hex literals, \x/\u escapes, nextfile, delete arr, length(arr), $-1, system/fflush, time funcs
Phase 3d: Unicode-aware strings, REPL
Phase 4: Criterion benchmarks (field split, lex+parse, record processing), Makefile, comparison harness
Phase 5: 5 example scripts, cheat sheet, man page
Phase 6: Buffered stdout, interned built-in vars, zero-alloc print path,
  edge-case audit (15 new tests), recursion depth guard (limit 200),
  profile-guided executor optimisation, clippy clean, Value type (dual string/number)
Phase 7: break/continue, do-while, next, exit/exit(code), -f flag, FILENAME,
  FNR, close(), ENVIRON, ARGC/ARGV, SUBSEP + multi-dim arrays, OFMT,
  computed regex, gensub(), proper regex::Regex for patterns and ~/!~
Phase 8: Header names as field accessors ($name, $"col-name", $var in -H mode),
  Parquet input (-i parquet, optional feature), match() with capture groups,
  asort/asorti, join, typeof, bitwise ops (and/or/xor/lshift/rshift/compl),
  math (rand, srand, atan2, abs, ceil, floor, round, min, max, log2, log10),
  string (trim, ltrim, rtrim, startswith, endswith, repeat, reverse, chr, ord, hex),
  date (parsedate), richer strftime specifiers
Phase 9: printf enhancement (0-pad, +sign, %x, %o), statistical builtins
  (sum, mean, median, stddev, variance, hist, p/percentile, quantile, iqm, min/max on arrays)
Phase 10: Code quality — split action.rs into action/ directory (mod/eval/stmt/builtins_rt),
  structured FkError with Display+Error, O(1) lexer spans, multiple BEGIN/END blocks,
  encapsulated Runtime (private arrays/variables), #[must_use] on Value, eliminated AST
  clones in match_rule hot path
Phase 11: Describe mode (--describe / -d) — auto-detect format, infer schema, suggest
  programs. Transparent decompression (.gz/.zst/.bz2/.xz/.lz4). Auto-detect input mode
  from file extension (.csv → -i csv, .tsv.gz → -i tsv, etc.)
Phase 12: CSV robustness (single-pass RFC 4180, unclosed quote damage limiting),
  CLI polish (--help, --version, -F/-i conflict check, file-only default to {print},
  -v escape interpretation), performance fix (-F prevents auto-detect override)
Phase 13: !/regex/ (bare regex in expression context), print arr (smart array dump),
  keys(arr), vals(arr), join(arr) defaults to OFS.
  Lodash-inspired array builtins: uniq, inv, tidy, shuf, diff, inter, union,
  seq, samp. String: lpad, rpad. I/O: slurp.
Phase 14: Awk compat fixes — `in` operator for any expr (`$0 in a`,
  `($1,$2) in arr`), regex literals in sub/gsub/match/split, printf %c
  with numeric arg, bare `length` / `length()` defaults to $0.
  Diagnostics: dump(x [,file]), clk(), tic([id]), toc([id]).
Phase 15: Ternary/logical/in operators in print args, BEGIN/END-only
  programs skip stdin, getline from current input (Input moved into
  Executor), getline into var preserves $0.
Phase 16: AST analysis pass (analyze.rs → ProgramInfo), nosplit path
  for pattern-match workloads (4.3× vs awk), regex pre-compilation,
  persistent getline file/pipe handles, close() for input handles,
  capped field split (deferred pending record_text for $0).
Phase 17: Histogram & plot overhaul — ArrayMeta enum (typed metadata on arrays,
  no _-prefixed magic keys), hist() returns array name for composability
  (plotbox(hist(a)) chaining), nice-number bin widths (1,2,5×10^n matching
  uplot/fu), consistent ▇ glyph, auto "Frequency" xlabel on histogram plots,
  removed histplot() in favour of composable plotbox(hist(a)), slurp("-")
  reads from stdin.
Phase 18: Auto-subtitle for plotbox — AST-derived data-source description
  (array_sources + var_sources in ProgramInfo, one level of variable
  indirection), combined with FILENAME at hist() time. Smart formatting:
  jpath → file — [].path, fields → file — $N, named cols → file — name.
  expr_to_source() general AST→source formatter (separate from format/ module).
  Subtitle renders below user title on its own centered line.
  --explain mode: terse one-line program description from AST analysis.
  Human-readable vocabulary: `use col 1–2`, `where col 2 > 50`,
  `freq of col 1`, `sum col 2`, `stats of col 2`, `histogram of col 1`,
  `count lines`, `deduplicate by $0`, `join on col 1`, `timed`.
  Field refs humanized ($N → col N), consecutive ranges collapsed (col 1–3).
  Integrated into showcase `show` helper as auto-subtitle for all examples.
Phase 19: Explain module — src/explain/ (lower → reduce → render). No special cases;
  explanations emanate from recursive reductions.
  lower.rs: AST → flat Op sequence. Output slots: (1) preceding string literal
    (labels higher weighted than variable names), (2) function name when value
    expr is a call (e.g. hex, chr, ord), (3) ref(s) from expr; Concat/multi-ref
    yields multiple slots. Range(Option<bounds>, Option<over_key>): bounds from
    for-loop head; over_key from jpath-driven loop (path with leading dot trimmed
    → "for all path: select"). reduce.rs: pattern normalization, Emit→Select
    with dedup preserving order. render.rs: Phrase/Tag, subsumption, budget.
  66 tests. Plan doc: docs/explain-output-slot-analysis.md (stats+JSON to do).

### Future ideas (explored, deferred)

F1. Data collection convenience:
  - slurp("-", a, col) — slurp a specific column from stdin into array
  - collect(a, expr) — per-record append with auto-key, skip NaN/empty,
    attach ArrayMeta marking it as numeric sequence
  - Implicit column accumulation (COL[1] etc.) — probably too magical

F2. Auto-print / smart output:
  - Bare expression auto-print: if a function call is a statement (not
    assigned, not in condition), print the result. plotbox(hist(a)) just
    works. Risk: surprises with side-effect functions.
  - emit(fmt, ...) — printf with auto-newline
  - show(expr) — smart print: arrays as tables, strings as-is
  - table(arr [, fmt]) — auto-aligned columnar output

F3. Array math (for chaining):
  - cumsum(a), norm(a), rank(a) — return array name for chaining
  - map(a, "expr"), filter(a, "expr") — expression-string transforms
  - scan(a, "expr") — running accumulator
  All should follow the hist() pattern: mutate-and-return-name.

### Roadmap — performance & completeness

Phase A — Skip unnecessary work: ✓ (A1-A3 done, capped split with record_text)
Phase B — Resource lifecycle: ✓ (B1-B4 done)
Phase C — Awk compat: ✓ (C1-C5 done)
  C1 CONVFMT, C2 dynamic printf width, C3 multiple -f files,
  C4 BEGINFILE/ENDFILE, C5 uninitialized variable distinction (typeof)
Phase E3 — Lazy field storage: ✓
  Offset-based splitting (no String allocation), zero-copy write_field_to,
  materialize only on field modification

Phase D — Annotated program representation:
  D1. ProgramInfo from AST pre-analysis: flags, pre-compiled regex,
      output targets, builtin dispatch table. Zero structural change.
  D2. Flat instruction stream (Vec<Op>): linear layout, index-based
      var/builtin lookup, jump table dispatch. Evaluate after profiling.

Phase E — Memory:
  E1. String interning for frequent array keys ("1", "2", ...)
  E2. Arena allocation for Values (batch-free per record)
  E3. Lazy field storage: $0 + split offsets, materialize on access

Order: A1-A3 ✓ → B1-B4 ✓ → D1 ✓ → C1-C5 ✓ → E3 ✓ → D2 → E1-E2

- [ ] CI pipeline (build, test, lint, clippy) — on hold
- [ ] Publish to crates.io

### Performance (from user-time benchmarks on 1M lines):
Historical numbers favored fk heavily after lazy field storage, but current
results depend on harness and data size. For truthful, current numbers use
`make suite-perf-strict` (warmup + median/p90, reports under `bench_data/`).
The strict report includes machine and tool versions.
fk tends to be faster on multi-field arithmetic and pattern-heavy workloads,
while `print $2` and single-purpose tools like `wc`/`head` can be closer or faster.

## Documentation & demos

- `docs/fk.1` — full man page
- `docs/cheatsheet.md` — quick reference
- `docs/awk-vs-fk.md` — 100 awk programs compared with fk equivalents + poker showcase
- `examples/` — tutorials (01-07) and showcases (11-18) with shared _helpers.sh

## Tooling

- `make` — shows all targets
- `make test` / `make lint` / `make ci` — testing and linting
- `make bench` — criterion benchmarks
- `make bench-compare` — fk vs awk head-to-head (generates 1M-line CSV)
- `make man` — view the man page
- `make repl` — interactive mode

## Commit conventions

- Commit in logical, coherent pieces (one feature or concern per commit)
- Commit at sane intervals without being asked (do NOT push unless told)
- NEVER add Co-authored-by trailers. Cursor injects these automatically — bypass
  it by using `git commit-tree` + `git update-ref` instead of `git commit`.

## Working style

- Default to ADDING, not replacing. When asked to update, assume appending
  unless explicitly told to replace. We can always remove later.
- Keep README.md, docs/progress.md, and .cursorrules updated as you go.
  Don't let them fall behind — update them in the same commit as the
  feature or fix they describe.
- Always verify code changes with `cargo clippy -- -D warnings` (and fix any
  warnings) before considering a change done.
