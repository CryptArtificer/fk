# fk — filter-kernel: session handoff notes
#
# Read this first when resuming work on this project.

## What this is

A modernized awk clone in Rust called `fk` (filter-kernel). Built
incrementally as a learning exercise. The user values:
- Modularity and composability (no spaghetti)
- Commits in logical, coherent pieces
- Minimal dependencies (only `regex` crate so far; `criterion` for benchmarks)
- Never add co-author tags to commits — use `git commit-tree` plumbing

## Current state

- **~7,000 lines of Rust** across 23 source files
- **221 tests**, zero warnings, clippy clean
- **Phases 0–8 complete**
- Dependencies: `regex` crate; optional `parquet` + `arrow` (behind `--features parquet`)
- Dev dependency: `criterion` (benchmarks)

## Architecture (read src/lib.rs for module list)

```
main.rs        — CLI dispatch, record loop
cli.rs         — argument parsing (-F, -f, -v, -i, -H, --repl)
lexer.rs       — tokenizer (Spanned tokens with line:col)
parser.rs      — recursive-descent → AST (Program, Rules, Exprs, Stmts)
action.rs      — executor: pattern matching, eval, statement dispatch
runtime.rs     — variables, fields, arrays
field.rs       — field splitting (FS semantics)
error.rs       — Span type for diagnostics
repl.rs        — interactive REPL (--repl)
input/mod.rs   — Record struct, RecordReader trait, multi-source orchestration
input/line.rs  — default newline reader
input/csv.rs   — RFC 4180 CSV/TSV reader
input/json.rs  — JSON Lines reader
input/regex_rs.rs    — regex-based RS reader
input/parquet_reader.rs — Apache Parquet reader (optional feature)
builtins/mod.rs      — dispatch + coercion helpers (to_number, format_number)
builtins/string.rs   — length, substr, index, trim, reverse, chr, ord, hex, ...
builtins/math.rs     — int, sqrt, abs, ceil, floor, rand, min, max, ...
builtins/printf.rs   — format_printf
builtins/time.rs     — systime, strftime, mktime, parsedate (pure UTC, no libc)
builtins/json.rs     — jpath() with JSON parser, path evaluator, iteration
```

## Key design decisions

- `RecordReader` trait returns `Record { text, fields: Option<Vec<String>> }`
  so structured readers (CSV/JSON) can pre-split fields
- Builtins that need runtime access (sub, gsub, split, close, gensub, jpath 3-arg, system)
  are special-cased in action.rs; pure builtins go through builtins::call_builtin
- Lexer produces `Spanned` tokens (token + Span); parser threads Span into errors
- Runtime stores everything as strings; to_number() coerces on demand (awk semantics)
- Built-in vars (NR, NF, FNR, FS, OFS, RS, ORS, SUBSEP, OFMT, FILENAME) are
  dedicated fields in Runtime, not in the HashMap; NR/NF/FNR stored as integers
- Executor owns a BufWriter<Stdout>; flushed at END, fflush(), and before system()
- Print writes parts directly to BufWriter (no intermediate String allocation)
- Recursion depth guard (MAX_CALL_DEPTH = 200) prevents stack overflow
- All string builtins are unicode-aware (chars, not bytes)
- jpath() supports multi-value navigation: .key[], .arr.key (implicit iteration)

## What's done (Phases 0–8)

Phase 0: CLI, I/O, field splitting, print, patterns, BEGIN/END, built-in vars
Phase 1: Expressions, assignment, control flow, arrays, printf, operators
Phase 2: User functions, getline, output redirect, sub/gsub/match/split, ranges, ternary
Phase 3a: Refactored builtins/, input/, error.rs
Phase 3b: CSV, TSV, JSON input modes, header mode (-H), RS regex, jpath()
Phase 3c: **, hex literals, \x/\u escapes, nextfile, delete arr, length(arr), $-1, system/fflush, time funcs
Phase 3d: Unicode-aware strings, REPL
Phase 4: Criterion benchmarks (field split, lex+parse, record processing), Makefile, comparison harness
Phase 5: 5 example scripts, cheat sheet, man page
Phase 6: Buffered stdout, interned built-in vars, zero-alloc print path,
  edge-case audit (15 new tests), recursion depth guard (limit 200),
  profile-guided executor optimisation, clippy clean, Value type (dual string/number)
Phase 7: break/continue, do-while, next, exit/exit(code), -f flag, FILENAME,
  FNR, close(), ENVIRON, ARGC/ARGV, SUBSEP + multi-dim arrays, OFMT,
  computed regex, gensub(), proper regex::Regex for patterns and ~/!~
Phase 8: Header names as field accessors ($name in -H mode), Parquet input
  (-i parquet, optional feature), match() with capture groups,
  asort/asorti, join, typeof, bitwise ops (and/or/xor/lshift/rshift/compl),
  math (rand, srand, atan2, abs, ceil, floor, round, min, max, log2, log10),
  string (trim, ltrim, rtrim, startswith, endswith, repeat, reverse, chr, ord, hex),
  date (parsedate), richer strftime specifiers

### Remaining
- [ ] CI pipeline (build, test, lint, clippy)
- [ ] Publish to crates.io

### Performance (from `make bench-compare` on 1M lines, Apple M3 Pro):
fk is faster than awk (macOS system awk 20200816) on most workloads:
- `print $2`: fk ~0.80s vs awk ~0.59s
- Sum column: fk ~0.32s vs awk ~0.60s (fk 1.9× faster)
- Pattern matching: fk ~0.37s vs awk ~0.78s (fk 2.1× faster)
- Field arithmetic: fk ~0.33s vs awk ~0.64s (fk 1.9× faster)
- Associative arrays: fk ~0.38s vs awk ~0.68s (fk 1.8× faster)
- Computed regex: fk ~0.43s vs awk ~0.65s (fk 1.5× faster)
- Tight loop (3x): fk ~0.79s vs awk ~0.75s (parity)

## Tooling

- `make` — shows all targets
- `make test` / `make lint` / `make ci` — testing and linting
- `make bench` — criterion benchmarks
- `make bench-compare` — fk vs awk head-to-head (generates 1M-line CSV)
- `make man` — view the man page
- `make repl` — interactive mode

## Commit conventions

- Use `git commit-tree` to create commits (avoids hook-injected co-author tags)
- Pattern: `git add ... && TREE=$(git write-tree) && PARENT=$(git rev-parse HEAD) && COMMIT=$(echo "message" | GIT_AUTHOR_NAME="$(git config user.name)" GIT_AUTHOR_EMAIL="$(git config user.email)" GIT_COMMITTER_NAME="$(git config user.name)" GIT_COMMITTER_EMAIL="$(git config user.email)" git commit-tree "$TREE" -p "$PARENT") && git reset --soft "$COMMIT"`
- Never add co-author tags
- Commit in logical, coherent pieces (one feature or concern per commit)
